{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f86741",
   "metadata": {},
   "source": [
    "### Demo for the MMML for Human Behavior Understanding\n",
    "Author: Xinyu Li\n",
    "\n",
    "ID: 2278619\n",
    "\n",
    "Date: July/2022\n",
    "\n",
    "Project Name: Multimodal Machine Learning for Human Behavior Understanding\n",
    "\n",
    "This repository are used to show the programming implementation of the MSc Individual Project (EESE MSc 2022 Summer) at the School of Engineering, University of Birmingham.\n",
    "\n",
    "Before Running: import all reqired package by !pip or manually conbine the ipykernal to a specific python environment (venv).\n",
    "The full install table is given in requirement.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614c7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import section'''\n",
    "import os\n",
    "import json\n",
    "# !pip install pymrmr\n",
    "import pymrmr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from smart_open import smart_open\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# !pip install tensorflow\n",
    "# !pip install tensorflow-estimator\n",
    "# from autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94671f3",
   "metadata": {},
   "source": [
    "##### Loading the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3837824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./dataset/prepocessed_feature/V_data_train.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_config = json.load(open('./config/address.json', 'r'))\n",
    "\n",
    "out_address = address_config['prepocessed_feature']['pre-aligned']\n",
    "print(os.path.isfile(out_address['train_Audio']))\n",
    "print(os.path.isfile(out_address['train_Visual']))\n",
    "out_address['train_Visual']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77c0b0",
   "metadata": {},
   "source": [
    "##### Loading the preprocessed data\n",
    "For audio data:          \n",
    "MFCC(39) + MFCC Velocity(39) + MFCC Acceleration(39) + 1 = 118\n",
    "\n",
    "For visual data:              \n",
    "Facial Landmarks_x(68) + Facial Landmarks_y(68) + eye gaze(6) + head pose(6) + AU_r(17) + AU_c(18) + 1 = 184 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed5f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-processed data detected, start loading the features ...\n",
      "=============================================\n",
      "Size of train_Audio (759575, 118)\n",
      "Size of train_Visual (759575, 184)\n",
      "Size of dev_Audio (317104, 118)\n",
      "Size of dev_Visual (317104, 184)\n",
      "Size of test_Audio (372734, 118)\n",
      "Size of test_Visual (372734, 184)\n",
      "---------------------------------------------\n",
      "Size of train_label (759576, 1)\n",
      "Size of dev_label (317105, 1)\n",
      "Size of train_instance (759576, 1)\n",
      "Size of dev_instance (317105, 1)\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(out_address['train_Audio']) and os.path.isfile(out_address['train_Visual']):\n",
    "    print(\"\\nPre-processed data detected, start loading the features ...\")\n",
    "    train_Audio = pd.read_csv(out_address['train_Audio'], header=None) \n",
    "    dev_Audio = pd.read_csv(out_address['dev_Audio'], header=None) \n",
    "    test_Audio = pd.read_csv(out_address['test_Audio'], header=None)\n",
    "    train_Visual = pd.read_csv(out_address['train_Visual'], header=None, low_memory=False)\n",
    "    dev_Visual = pd.read_csv(out_address['dev_Visual'], header=None, low_memory=False) \n",
    "    test_Visual = pd.read_csv(out_address['test_Visual'], header=None, low_memory=False)\n",
    "    train_label = pd.read_csv(out_address['train_label'], header=None) \n",
    "    train_instance = pd.read_csv(out_address['train_instance'], header=None) \n",
    "    dev_label = pd.read_csv(out_address['dev_label'], header=None) \n",
    "    dev_instance = pd.read_csv(out_address['dev_instance'], header=None)\n",
    "\n",
    "    print(\"===\" * 15)\n",
    "    print(\"Size of train_Audio\", train_Audio.shape)\n",
    "    print(\"Size of train_Visual\", train_Visual.shape)\n",
    "    print(\"Size of dev_Audio\", dev_Audio.shape)\n",
    "    print(\"Size of dev_Visual\", dev_Visual.shape)\n",
    "    print(\"Size of test_Audio\", test_Audio.shape)\n",
    "    print(\"Size of test_Visual\", test_Visual.shape)\n",
    "    print(\"---\" * 15)\n",
    "    print(\"Size of train_label\", train_label.T.shape)\n",
    "    print(\"Size of dev_label\", dev_label.T.shape)\n",
    "    print(\"Size of train_instance\", train_instance.T.shape)\n",
    "    print(\"Size of dev_instance\", dev_instance.T.shape)\n",
    "    print(\"===\" * 15)\n",
    "else:\n",
    "    print(\"Successfully loading all pre-preproposed features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5bf4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first column (time-frame index)\n",
    "train_Audio = train_Audio.iloc[:,1:]  \n",
    "dev_Audio = dev_Audio.iloc[:,1:]\n",
    "test_Audio = test_Audio.iloc[:,1:]\n",
    "train_Visual = train_Visual.iloc[:,1:]\n",
    "dev_Visual = dev_Visual.iloc[:,1:]\n",
    "test_Visual = test_Visual.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92862dab",
   "metadata": {},
   "source": [
    "##### Loading the modal config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8510e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loading the DDAE configuration.\n"
     ]
    }
   ],
   "source": [
    "# from autoencoder:\n",
    "model_config = json.load(open('./config/model.json', 'r'))['DDAE']\n",
    "dest_path = model_config['dest_path']\n",
    "hidden_ratio = model_config['hidden_ratio']\n",
    "learning_rate = model_config['learning_rate']\n",
    "batch_size = model_config['batch_size']\n",
    "epochs = model_config['epoch_number']\n",
    "noise = model_config['noise_rate']\n",
    "p = model_config['p']\n",
    "beta = model_config['beta']\n",
    "\n",
    "print(\"\\nSuccessfully loading the DDAE configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a124652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Multi-DDAE model\n",
    "name = '%s_hidden%.2f_batch%d_epoch%d_noise%s' % ('multimodal_aligned_mfcc', hidden_ratio, batch_size, epochs, noise)\n",
    "dim_ori_Audio =  117 # 39*3\n",
    "dim_ori_Visual_1 = 136\n",
    "dim_ori_Visual_2 = 6\n",
    "dim_ori_Visual_3 = 6\n",
    "dim_ori_Visual_4 = 35\n",
    "dim_ori_Visual = dim_ori_Visual_1 + dim_ori_Visual_2 + dim_ori_Visual_3 + dim_ori_Visual_4 # = 183\n",
    "decoder_Audio = None\n",
    "decoder_Visual_1 = None\n",
    "decoder_Visual_2 = None\n",
    "decoder_Visual_3 = None\n",
    "decoder_Visual_4 = None\n",
    "# dim_ori_Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83b129",
   "metadata": {},
   "source": [
    "##### Consturcting the input layer\n",
    "Function called from: Keras Engine Input layer.\n",
    "\n",
    "Please install Keras==2.2.4 Keras-Applications==1.0.7 keras-metrics==1.1.0 Keras-Preprocessing==1.0.9 \n",
    "\n",
    "(Source: https://github.com/keras-team/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57d35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Multi-DDAE model\n",
    "\n",
    "if not os.path.isdir(os.path.join(dest_path, name)):\n",
    "    os.mkdir(os.path.join(dest_path, name))\n",
    "    already_fitted = False\n",
    "else:\n",
    "    already_fitted = True\n",
    "\n",
    "if hidden_ratio != 1.0:\n",
    "    dim_Audio = int(dim_ori_Audio * hidden_ratio)\n",
    "    dim_V1 = int(dim_ori_Visual_1 * hidden_ratio)\n",
    "    dim_V2 = int(dim_ori_Visual_2 * hidden_ratio)\n",
    "    dim_V3 = int(dim_ori_Visual_3 * hidden_ratio)\n",
    "    dim_V4 = int(dim_ori_Visual_4 * hidden_ratio)\n",
    "    dim = int((dim_ori_Audio + dim_ori_Visual) * hidden_ratio / 4)\n",
    "    \n",
    "input_Audio = Input(shape=(dim_ori_Audio, ), name='audio_MFCCs')\n",
    "input_Video_1 = Input(shape=(dim_ori_Visual_1, ), name='video_landmark')\n",
    "input_Video_2 = Input(shape=(dim_ori_Visual_2, ), name='video_eyegaze')\n",
    "input_Video_3 = Input(shape=(dim_ori_Visual_3, ), name='video_headpose')\n",
    "input_Video_4 = Input(shape=(dim_ori_Visual_4, ), name='video_AU')\n",
    "encoded_input = Input(shape=(dim, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fc446",
   "metadata": {},
   "source": [
    "##### Consturcting the deep autoencoder\n",
    "Dense function called from: Core Keras Layers.\n",
    "\n",
    "Concatenate function called from: Merge Layers. \n",
    "\n",
    "Model function called from: Training-related part of the Keras engine.\n",
    "\n",
    "Please install Keras==2.2.4 Keras-Applications==1.0.7 keras-metrics==1.1.0 Keras-Preprocessing==1.0.9 \n",
    "\n",
    "(Source: https://github.com/keras-team/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462dce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_layer_Audio = Dense(dim_Audio, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='audio_MFCCs_encoded')(input_Audio)\n",
    "encode_layer_Video_1 = Dense(dim_V1, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_landmark_encoded')(input_Video_1)\n",
    "encode_layer_Video_2 = Dense(dim_V2, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_eyegaze_encoded')(input_Video_2)\n",
    "encode_layer_Video_3 = Dense(dim_V3,\n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_headpose_encoded')(input_Video_3)\n",
    "encode_layer_Video_4 = Dense(dim_V4, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_AU_encoded')(input_Video_4)\n",
    "\n",
    "shared_layer = Concatenate(axis=1, name='concat')([encode_layer_Audio, encode_layer_Video_1, encode_layer_Video_2, encode_layer_Video_3, encode_layer_Video_4])\n",
    "\n",
    "encoded_layer = Dense(dim, \n",
    "            activation='relu',\n",
    "            name='shared_repres')(shared_layer)\n",
    "\n",
    "decode_layer_Audio = Dense(dim_Audio, \n",
    "                activation='relu', name='audio_MFCCs_decoded')(encoded_layer)\n",
    "decode_layer_Video_1 = Dense(dim_V1, \n",
    "                activation='relu', name='video_landmark_decoded')(encoded_layer)\n",
    "decode_layer_Video_2 = Dense(dim_V2, \n",
    "                activation='relu', name='video_eyegaze_decoded')(encoded_layer)\n",
    "decode_layer_Video_3 = Dense(dim_V3, \n",
    "                activation='relu', name='video_headpose_decoded')(encoded_layer)\n",
    "decode_layer_Video_4 = Dense(dim_V4, \n",
    "                activation='relu', name='video_AU_decoded')(encoded_layer)\n",
    "\n",
    "decode_layer_Audio = Dense(dim_ori_Audio, activation='linear',\n",
    "                name='audio_MFCCs_reconstructed')(decode_layer_Audio)\n",
    "decode_layer_Video_1 = Dense(dim_ori_Visual_1, activation='linear',\n",
    "                name='video_landmark_reconstructed')(decode_layer_Video_1)\n",
    "decode_layer_Video_2 = Dense(dim_ori_Visual_2, activation='linear',\n",
    "                name='video_eyegaze_reconstructed')(decode_layer_Video_2)\n",
    "decode_layer_Video_3 = Dense(dim_ori_Visual_3, activation='linear',\n",
    "                name='video_headpose_reconstructed')(decode_layer_Video_3)\n",
    "decode_layer_Video_4 = Dense(dim_ori_Visual_4, activation='linear',\n",
    "                name='video_AU_reconstructed')(decode_layer_Video_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be96af52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Summary of the deep autoencoder: \n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " audio_MFCCs (InputLayer)       [(None, 117)]        0           []                               \n",
      "                                                                                                  \n",
      " video_landmark (InputLayer)    [(None, 136)]        0           []                               \n",
      "                                                                                                  \n",
      " video_eyegaze (InputLayer)     [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " video_headpose (InputLayer)    [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " video_AU (InputLayer)          [(None, 35)]         0           []                               \n",
      "                                                                                                  \n",
      " audio_MFCCs_encoded (Dense)    (None, 58)           6844        ['audio_MFCCs[0][0]']            \n",
      "                                                                                                  \n",
      " video_landmark_encoded (Dense)  (None, 68)          9316        ['video_landmark[0][0]']         \n",
      "                                                                                                  \n",
      " video_eyegaze_encoded (Dense)  (None, 3)            21          ['video_eyegaze[0][0]']          \n",
      "                                                                                                  \n",
      " video_headpose_encoded (Dense)  (None, 3)           21          ['video_headpose[0][0]']         \n",
      "                                                                                                  \n",
      " video_AU_encoded (Dense)       (None, 17)           612         ['video_AU[0][0]']               \n",
      "                                                                                                  \n",
      " concat (Concatenate)           (None, 149)          0           ['audio_MFCCs_encoded[0][0]',    \n",
      "                                                                  'video_landmark_encoded[0][0]', \n",
      "                                                                  'video_eyegaze_encoded[0][0]',  \n",
      "                                                                  'video_headpose_encoded[0][0]', \n",
      "                                                                  'video_AU_encoded[0][0]']       \n",
      "                                                                                                  \n",
      " shared_repres (Dense)          (None, 37)           5550        ['concat[0][0]']                 \n",
      "                                                                                                  \n",
      " audio_MFCCs_decoded (Dense)    (None, 58)           2204        ['shared_repres[0][0]']          \n",
      "                                                                                                  \n",
      " video_landmark_decoded (Dense)  (None, 68)          2584        ['shared_repres[0][0]']          \n",
      "                                                                                                  \n",
      " video_eyegaze_decoded (Dense)  (None, 3)            114         ['shared_repres[0][0]']          \n",
      "                                                                                                  \n",
      " video_headpose_decoded (Dense)  (None, 3)           114         ['shared_repres[0][0]']          \n",
      "                                                                                                  \n",
      " video_AU_decoded (Dense)       (None, 17)           646         ['shared_repres[0][0]']          \n",
      "                                                                                                  \n",
      " audio_MFCCs_reconstructed (Den  (None, 117)         6903        ['audio_MFCCs_decoded[0][0]']    \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " video_landmark_reconstructed (  (None, 136)         9384        ['video_landmark_decoded[0][0]'] \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " video_eyegaze_reconstructed (D  (None, 6)           24          ['video_eyegaze_decoded[0][0]']  \n",
      " ense)                                                                                            \n",
      "                                                                                                  \n",
      " video_headpose_reconstructed (  (None, 6)           24          ['video_headpose_decoded[0][0]'] \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " video_AU_reconstructed (Dense)  (None, 35)          630         ['video_AU_decoded[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 44,991\n",
      "Trainable params: 44,991\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Summary of the encoder: \n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " audio_MFCCs (InputLayer)       [(None, 117)]        0           []                               \n",
      "                                                                                                  \n",
      " video_landmark (InputLayer)    [(None, 136)]        0           []                               \n",
      "                                                                                                  \n",
      " video_eyegaze (InputLayer)     [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " video_headpose (InputLayer)    [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " video_AU (InputLayer)          [(None, 35)]         0           []                               \n",
      "                                                                                                  \n",
      " audio_MFCCs_encoded (Dense)    (None, 58)           6844        ['audio_MFCCs[0][0]']            \n",
      "                                                                                                  \n",
      " video_landmark_encoded (Dense)  (None, 68)          9316        ['video_landmark[0][0]']         \n",
      "                                                                                                  \n",
      " video_eyegaze_encoded (Dense)  (None, 3)            21          ['video_eyegaze[0][0]']          \n",
      "                                                                                                  \n",
      " video_headpose_encoded (Dense)  (None, 3)           21          ['video_headpose[0][0]']         \n",
      "                                                                                                  \n",
      " video_AU_encoded (Dense)       (None, 17)           612         ['video_AU[0][0]']               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concat (Concatenate)           (None, 149)          0           ['audio_MFCCs_encoded[0][0]',    \n",
      "                                                                  'video_landmark_encoded[0][0]', \n",
      "                                                                  'video_eyegaze_encoded[0][0]',  \n",
      "                                                                  'video_headpose_encoded[0][0]', \n",
      "                                                                  'video_AU_encoded[0][0]']       \n",
      "                                                                                                  \n",
      " shared_repres (Dense)          (None, 37)           5550        ['concat[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,364\n",
      "Trainable params: 22,364\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Summary of the audio decoder: \n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 37)]              0         \n",
      "                                                                 \n",
      " audio_MFCCs_decoded (Dense)  (None, 58)               2204      \n",
      "                                                                 \n",
      " audio_MFCCs_reconstructed (  (None, 117)              6903      \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,107\n",
      "Trainable params: 9,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "===================================================================================================\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "encode_layer_Audio = Dense(dim_Audio, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='audio_MFCCs_encoded')(input_Audio)\n",
    "encode_layer_Video_1 = Dense(dim_V1, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_landmark_encoded')(input_Video_1)\n",
    "encode_layer_Video_2 = Dense(dim_V2, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_eyegaze_encoded')(input_Video_2)\n",
    "encode_layer_Video_3 = Dense(dim_V3,\n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_headpose_encoded')(input_Video_3)\n",
    "encode_layer_Video_4 = Dense(dim_V4, \n",
    "                activation='relu', kernel_initializer='he_uniform',\n",
    "                name='video_AU_encoded')(input_Video_4)\n",
    "\n",
    "shared_layer = Concatenate(axis=1, name='concat')([encode_layer_Audio, encode_layer_Video_1, encode_layer_Video_2, encode_layer_Video_3, encode_layer_Video_4])\n",
    "\n",
    "encoded_layer = Dense(dim, \n",
    "            activation='relu',\n",
    "            name='shared_repres')(shared_layer)\n",
    "\n",
    "decode_layer_Audio = Dense(dim_Audio, \n",
    "                activation='relu', name='audio_MFCCs_decoded')(encoded_layer)\n",
    "decode_layer_Video_1 = Dense(dim_V1, \n",
    "                activation='relu', name='video_landmark_decoded')(encoded_layer)\n",
    "decode_layer_Video_2 = Dense(dim_V2, \n",
    "                activation='relu', name='video_eyegaze_decoded')(encoded_layer)\n",
    "decode_layer_Video_3 = Dense(dim_V3, \n",
    "                activation='relu', name='video_headpose_decoded')(encoded_layer)\n",
    "decode_layer_Video_4 = Dense(dim_V4, \n",
    "                activation='relu', name='video_AU_decoded')(encoded_layer)\n",
    "\n",
    "decode_layer_Audio = Dense(dim_ori_Audio, activation='linear',\n",
    "                name='audio_MFCCs_reconstructed')(decode_layer_Audio)\n",
    "decode_layer_Video_1 = Dense(dim_ori_Visual_1, activation='linear',\n",
    "                name='video_landmark_reconstructed')(decode_layer_Video_1)\n",
    "decode_layer_Video_2 = Dense(dim_ori_Visual_2, activation='linear',\n",
    "                name='video_eyegaze_reconstructed')(decode_layer_Video_2)\n",
    "decode_layer_Video_3 = Dense(dim_ori_Visual_3, activation='linear',\n",
    "                name='video_headpose_reconstructed')(decode_layer_Video_3)\n",
    "decode_layer_Video_4 = Dense(dim_ori_Visual_4, activation='linear',\n",
    "                name='video_AU_reconstructed')(decode_layer_Video_4)\n",
    "\n",
    "# Training model of the deep autoencoder\n",
    "deep_autoencoder = Model(inputs=[input_Audio, \n",
    "                    input_Video_1, input_Video_2, input_Video_3, input_Video_4], \n",
    "                    outputs=[decode_layer_Audio, \n",
    "                    decode_layer_Video_1, decode_layer_Video_2, decode_layer_Video_3, decode_layer_Video_4])\n",
    "encoder = Model(inputs=[input_Audio, \n",
    "                    input_Video_1, input_Video_2, input_Video_3, input_Video_4], \n",
    "                    outputs=encoded_layer)\n",
    "decoder_Audio = Model(inputs=encoded_input, \n",
    "                    outputs=deep_autoencoder.get_layer('audio_MFCCs_reconstructed')(\n",
    "                        deep_autoencoder.get_layer('audio_MFCCs_decoded')(\n",
    "                        encoded_input)))\n",
    "decoder_Video_1 = Model(inputs=encoded_input, \n",
    "                    outputs=deep_autoencoder.get_layer('video_landmark_reconstructed')(\n",
    "                        deep_autoencoder.get_layer('video_landmark_decoded')(\n",
    "                        encoded_input)))\n",
    "decoder_Video_2 = Model(inputs=encoded_input, \n",
    "                    outputs=deep_autoencoder.get_layer('video_eyegaze_reconstructed')(\n",
    "                        deep_autoencoder.get_layer('video_eyegaze_decoded')(\n",
    "                        encoded_input)))\n",
    "decoder_Video_3 = Model(inputs=encoded_input, \n",
    "                    outputs=deep_autoencoder.get_layer('video_headpose_reconstructed')(\n",
    "                        deep_autoencoder.get_layer('video_headpose_decoded')(\n",
    "                        encoded_input)))\n",
    "decoder_Video_4 = Model(inputs=encoded_input, \n",
    "                    outputs=deep_autoencoder.get_layer('video_AU_reconstructed')(\n",
    "                        deep_autoencoder.get_layer('video_AU_decoded')(\n",
    "                        encoded_input)))\n",
    "\n",
    "# configure model\n",
    "# two combo ['adam' + 'mse] ['adadelta', 'binary_crossentropy']\n",
    "deep_autoencoder.compile(optimizer='adam', \n",
    "                        loss='mse',\n",
    "                        metrics=[metrics.mse, metrics.mse,\n",
    "                                metrics.mse, metrics.mse,\n",
    "                                metrics.mse],\n",
    "                        loss_weights=[0.35, 0.35, 0.1, 0.1, 0.1])\n",
    "print(\"===\" * 33)\n",
    "print(\"Summary of the deep autoencoder: \")\n",
    "print(deep_autoencoder.summary())\n",
    "print(\"---\" * 33)\n",
    "print(\"Summary of the encoder: \")\n",
    "print(encoder.summary())\n",
    "print(\"---\" * 33)\n",
    "print(\"Summary of the audio decoder: \")\n",
    "print(decoder_Audio.summary())\n",
    "print(\"===\" * 33)\n",
    "\n",
    "# display the model structure by graphviz: \n",
    "plot_model(deep_autoencoder, show_shapes=True, to_file=os.path.join(dest_path, name, 'multimodal_DDAE.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af624659",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### Training Stage \n",
    "\n",
    "Fit function called from: Training-related part of the Keras engine.\n",
    "\n",
    "Please install Keras==2.2.4 Keras-Applications==1.0.7 keras-metrics==1.1.0 Keras-Preprocessing==1.0.9 \n",
    "\n",
    "(Source: https://github.com/keras-team/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a968e0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Existing model has already fitted. \n",
      "Open model: multimodal_aligned_mfcc_hidden0.50_batch1024_epoch100_noise0.1\n",
      "\n",
      "Completed loading the weights of fitted model. \n",
      "Processing model: multimodal_aligned_mfcc_hidden0.50_batch1024_epoch100_noise0.1\n"
     ]
    }
   ],
   "source": [
    "if already_fitted:\n",
    "    print(\"\\nExisting model has already fitted. \\nOpen model:\", name)\n",
    "    deep_autoencoder.load_weights(os.path.join(dest_path, name, 'DDAE.h5'))\n",
    "    print(\"\\nCompleted loading the weights of fitted model. \\nProcessing model:\", name)\n",
    "else:\n",
    "    train_Video_1, train_Video_2, train_Video_3, train_Video_4 = separate_V(train_Video)\n",
    "    dev_Video_1, dev_Video_2, dev_Video_3, dev_Video_4 = separate_V(dev_Video)\n",
    "\n",
    "    train_Audio = np.vstack((train_Audio, X_dev_A))\n",
    "    train_Video_1 = np.vstack((train_Video_1, dev_Video_1))\n",
    "    train_Video_2 = np.vstack((train_Video_2, dev_Video_2))\n",
    "    train_Video_3 = np.vstack((train_Video_3, dev_Video_3))\n",
    "    train_Video_4 = np.vstack((train_Video_4, dev_Video_4))\n",
    "\n",
    "    if noisy:\n",
    "        train_Audio_noisy = add_noise(train_Audio, noise)\n",
    "        train_Video_1_noisy = add_noise(train_Video_1, noise)\n",
    "        train_Video_2_noisy = add_noise(train_Video_2, noise)\n",
    "        train_Video_3_noisy = add_noise(train_Video_3, noise)\n",
    "        train_Video_4_noisy = add_noise(train_Video_4, noise)\n",
    "    else:\n",
    "        train_Audio_noisy = train_Audio\n",
    "        train_Video_1_noisy = train_Video_1\n",
    "        train_Video_2_noisy = train_Video_2\n",
    "        train_Video_3_noisy = train_Video_3\n",
    "        train_Video_4_noisy = train_Video_4\n",
    "\n",
    "    assert train_Audio_noisy.shape == train_Audio.shape\n",
    "    assert train_Video_1_noisy.shape == train_Video_1.shape\n",
    "    assert train_Video_2_noisy.shape == train_Video_2.shape\n",
    "    assert train_Video_3_noisy.shape == train_Video_3.shape\n",
    "    assert train_Video_4_noisy.shape == train_Video_4.shape\n",
    "    \n",
    "    # save the training procedure to the local logger (logger.csv)\n",
    "#     csv_logger = CSVLogger(os.path.join(dest_path, name, \"logger.csv\"))\n",
    "#     checkpoint = ModelCheckpoint(os.path.join(dest_path, name, \"weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"), monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#     callbacks_list = [csv_logger, checkpoint]\n",
    "\n",
    "    deep_autoencoder.fit([train_Audio_noisy, \n",
    "                        train_Video_1_noisy, train_Video_2_noisy, \n",
    "                        train_Video_3_noisy, train_Video_4_noisy],\n",
    "                        [train_Audio, train_Video_1, train_Video_2,\n",
    "                        train_Video_3, train_Video_4],\n",
    "                        epochs=epoch_number,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        callbacks=callbacks_list)\n",
    "    print(\"\\nThe training stage is finished, save to: \", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dd33b",
   "metadata": {},
   "source": [
    "##### Check the training results\n",
    "For the training log-book --- open file: ./pre-trained/DDAE/\"model_name\"/logger.csv\n",
    "\n",
    "For the auto-generated test result --- open file: ./pre-trained/DDAE/\"model_name\"/\"model_name\"_result.txt\n",
    "\n",
    "For the graphviz generated network structure --- open file: ./pre-trained/DDAE/\"model_name\"/multimodal_DDAE.png\n",
    "\n",
    "Meanwhile, all trained weights are given in the *.npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb8427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
